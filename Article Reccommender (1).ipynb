{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"We do not have to write a custom code for counting words and representing those counts as a vector. Scikit's CountVectorizer \n",
    "does the job very efficiently.\"\"\"\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1) #The parameter min_df determines how CountVectorizer treats words that are not used \n",
    "#frequently (minimum document frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just a dummy example to show how CountVectorizer creates vectors based on the frequency of words in a post.\n",
    "content = [\"How to format my hard disk\", \"Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "#The vectorizer detected seven words for which we can fetch the counts individually as follows:\n",
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Playing with a toy dataset. In this post dataset, we want to find the most similar post for the short post \"imaging databases\".\n",
    "#Assuming that the posts are located in 'D:\\ToStudy\\Machine Learning\\Data\\DIR', we can feed CountVectorizer with it as follows:\n",
    "\n",
    "posts = [open(os.path.join('D:\\ToStudy\\Machine Learning\\Data\\DIR', f)).read() for f in \n",
    "         os.listdir('D:\\ToStudy\\Machine Learning\\Data\\DIR')]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "#Notifying the vectorizer about the full dataset so that it knows upfront what words are to be expected:\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print (\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "#Unsurprisingly, we have five posts with a total of 25 different words. The following are the tokenized words which will be \n",
    "#counted.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can vectorize our new post as follows:\n",
    "\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Note that the count vectors returned by the 'transform' method are sparse. That is, each vector does not store one count\n",
    "value for each word, as most fo those counts would be zero (post does not contain the word). Instead, it uses the more memory \n",
    "efficient implementation 'coo_matrix' (for 'COOrdinate'). Our new post, for instance, actually contains only two elements:\"\"\"\n",
    "\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Via its member 'toarray()', we can gain access to the full ndarray as follows:\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"We need to use full array if we want to use it as a vector for similarity calculations.For the simialrity measurement(the \n",
    "naive one),we calculate the Eucledian distance between the count vectors of the new post and all the old posts as follows:\"\"\"\n",
    "\n",
    "import scipy as sp\n",
    "def dist(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray()) # The 'norm()' function calculates the Eucledian norm (shortest distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 4.00: This is a toy post about Machine Learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 2.00: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist = 1.41: Imaging databases store data.\n",
      "=== Post 4 with dist = 5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      ">>> Best post is 3 with dist = 1.41\n"
     ]
    }
   ],
   "source": [
    "#With 'dist()', we just need to iterate over all the posts and remember the nearest one:\n",
    "\n",
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist = %.2f: %s\"%(i, d, post)\n",
    "    \n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\">>> Best post is %i with dist = %.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"All the measurements make sense except posts 3 and 4. Post 4 is the same as Post 3, duplicated 3 times. So, it should be of \n",
    "the same similarity to the new post as Post 3.\n",
    "Printing the corresponding feature vectors explains the reason:\"\"\"\n",
    "\n",
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Obviously, using only the counts of the raw words is too simple. We will have to normalize them to get vectors of unit length.\n",
    "#We will have to extend 'dist()' to calculate the vector distance, not on the raw vectors but on the normalized ones instead:\n",
    "\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about Machine Learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.92: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      ">>> Best post is 3 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "#With 'dist_norm()', we just need to iterate over all the posts and remember the nearest one.\n",
    "\n",
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist = %.2f: %s\"%(i, d, post)\n",
    "    \n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\">>> Best post is %i with dist = %.2f\"%(best_i, best_dist))\n",
    "\n",
    "#This leads to the following similarity measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This looks a bit better now. Post 3 and Post 4 are calculated as being equally similar. One could argue whether that much \n",
    "#repetition would be a delight to the reader, but from the point of view of counting the words in the posts, this seems to be \n",
    "#right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bill',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'co',\n",
       " 'con',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'do',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fify',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Let us have another look at Post 2. Of its words that are not in the new post, we have 'most', 'safe', 'images', and \n",
    "'permanently'. They are actually quite different in the overall importnace to the post.\n",
    "\n",
    "Words such as 'most' appear very often in all sorts of different context, and words such as this are called STOP WORDS. They \n",
    "do not carry as much information, and thus should not be weighed as much as words such as 'images', that don't occur often in \n",
    "different contexts. \n",
    "\n",
    "The best option would be to remove all words that are so frequent that they do not help to distinguish between different texts.\n",
    "These words are called STOP WORDS.\"\"\"\n",
    "\n",
    "#As this is such a common step in text processing, there is a simple parameter in CountVectorizer to achieve this, as follows:\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "\"\"\"If you have a clear picture of what kind os top words you would want to remove, you can also pass a list of them. Setting \n",
    "'stop_words' to 'english' will use a set of 318 English stop words. We can find out these stop words by using \n",
    "'get_stop_words()':\"\"\"\n",
    "\n",
    "sorted(vectorizer.get_stop_words())[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 18\n"
     ]
    }
   ],
   "source": [
    "posts = [open(os.path.join('D:\\ToStudy\\Machine Learning\\Data\\DIR', f)).read() for f in \n",
    "         os.listdir('D:\\ToStudy\\Machine Learning\\Data\\DIR')]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "X_train_new = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train_new.shape\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "print (\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "#The new word list is seven words lighter, thanks to the removal of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about Machine Learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      ">>> Best post is 3 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train_new.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist = %.2f: %s\"%(i, d, post)\n",
    "    \n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\">>> Best post is %i with dist = %.2f\"%(best_i, best_dist))\n",
    "\n",
    "#Without stop words, we arrive at the following similarity measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Post 2 is now on par with Post 1. Overall, it has, however, not changed much as our posts are kept short for demostration \\npurposes. It will become vital when we look at real-world data.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Post 2 is now on par with Post 1. Overall, it has, however, not changed much as our posts are kept short for demostration \n",
    "purposes. It will become vital when we look at real-world data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"One thing is still missing. We count similar words in different variants as different words. Post 2, for instance,contains \n",
    "'imaging' and 'images'. It would make sense to count them together. After all, it is the same concept they are referring to.\n",
    "\n",
    "We need a function that reduces words to their specific word stem. Scikit does not contain a stemmer by default.With the NATURAL \n",
    "LANGUAGE TOOLKIT (NLTK), we can download a free software toolkit, which provides a stemmer that we can easily plug into \n",
    "'CountVectorizer'.\"\"\"\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLTK comes with different stemmers. This is necessary, because every language has a different set of rules for stemming.\n",
    "#For English, we can take 'SnowballStemmer'.\n",
    "\n",
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "#A few examples\n",
    "\n",
    "print s.stem(\"graphics\")\n",
    "\n",
    "print s.stem(\"imaging\")\n",
    "\n",
    "print s.stem(\"image\")\n",
    "\n",
    "print s.stem(\"imagination\")\n",
    "\n",
    "print s.stem(\"imagine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n",
      "buy\n",
      "bought\n"
     ]
    }
   ],
   "source": [
    "#It also works with verbs as follows:\n",
    "\n",
    "print s.stem(\"buys\")\n",
    "\n",
    "print s.stem(\"buying\")\n",
    "\n",
    "print s.stem(\"bought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n",
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'safe', u'storag', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "vectorizer = StemmedCountVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "\"\"\"This will perform the following steps for each post:\n",
    "1. Lower casing the raw post in the preprocessing step (done in the parent class).\n",
    "2. Extracting all individual words in the tokenizing step (done in the parent class).\n",
    "3. Converting each word into its stemmed version.\"\"\"\n",
    "\n",
    "X_train_new = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train_new.shape\n",
    "\n",
    "#As a result, we now have one feature less, because 'images' and 'imaging' collapsed to one.\n",
    "print (\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about Machine Learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.63: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      ">>> Best post is 2 with dist = 0.63\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train_new.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist = %.2f: %s\"%(i, d, post)\n",
    "    \n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\">>> Best post is %i with dist = %.2f\"%(best_i, best_dist))\n",
    "\n",
    "#Without stop words, we arrive at the following similarity measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Now, we would also like to assign higher importance to those terms which occur often in an article and very rarely anywhere \n",
    "else since this means that the term holds special value to that particular article. This can only be solved by counting term \n",
    "frequencies for every post, and in addition, discounting those that appear in many posts.\"\"\"\n",
    "\n",
    "\"\"\"This is exactly what TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY (TF - IDF) does; TF stands for counting part, while IDF \n",
    "factors in the discounting.\"\"\"\n",
    "\n",
    "#A simple implementation for the caluclation for tf-idf: \n",
    "import scipy as sp\n",
    "import math\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term)) / sum(doc.count(term) for w in docset)\n",
    "    idf = math.log(float(len(docset)) / (len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.135155036036\n",
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "#An example of tf-idf calculation\n",
    "\n",
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a,abb, abc]\n",
    "\n",
    "print(tfidf(\"a\", a, D))\n",
    "\n",
    "print(tfidf(\"a\", abc, D))\n",
    "\n",
    "print(tfidf(\"b\", abc, D))\n",
    "\n",
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about Machine Learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist = 0.92: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      ">>> Best post is 2 with dist = 0.86\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We see that 'a' carries no meaning for any document since it is contained everywhere. 'c' is more important to 'abc' than 'b' \n",
    "is to 'abb' since 'c' occurs only once in only one of the document viz., 'abc'.\"\"\"\n",
    "\n",
    "\"\"\"Scikit already has a very efficient implementation of tfidf calculation in 'TfidfVectorizer', which is inherited from \n",
    "'CountVectorizer'. We will include this in our stemmer as follows: \"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "vectorizer = StemmedTfidfVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "X_train_new = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train_new.shape\n",
    "\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train_new.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist = %.2f: %s\"%(i, d, post)\n",
    "    \n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\">>> Best post is %i with dist = %.2f\"%(best_i, best_dist))\n",
    "\n",
    "#Without stop words, we arrive at the following similarity measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'D:\\\\ToStudy\\\\Machine Learning\\\\Data\\\\project_data\\\\379\\\\raw\\\\comp.graphics\\\\1190-38614'\n",
      " 'D:\\\\ToStudy\\\\Machine Learning\\\\Data\\\\project_data\\\\379\\\\raw\\\\comp.graphics\\\\1383-38616'\n",
      " 'D:\\\\ToStudy\\\\Machine Learning\\\\Data\\\\project_data\\\\379\\\\raw\\\\alt.atheism\\\\487-53344'\n",
      " ...,\n",
      " 'D:\\\\ToStudy\\\\Machine Learning\\\\Data\\\\project_data\\\\379\\\\raw\\\\rec.sport.hockey\\\\10215-54303'\n",
      " 'D:\\\\ToStudy\\\\Machine Learning\\\\Data\\\\project_data\\\\379\\\\raw\\\\sci.crypt\\\\10799-15660'\n",
      " 'D:\\\\ToStudy\\\\Machine Learning\\\\Data\\\\project_data\\\\379\\\\raw\\\\comp.os.ms-windows.misc\\\\2732-10871']\n"
     ]
    }
   ],
   "source": [
    "#Loading the Data Set\n",
    "import sklearn.datasets\n",
    "MLCOMP_DIR = r\"D:\\ToStudy\\Machine Learning\\Data\\project_data\"\n",
    "data = sklearn.datasets.load_mlcomp(\"20news-18828\", mlcomp_root=MLCOMP_DIR)\n",
    "print(data.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18828\n"
     ]
    }
   ],
   "source": [
    "# Printing Total Number of Articles\n",
    "print(len(data.filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13180\n"
     ]
    }
   ],
   "source": [
    "#Training Set \n",
    "train_data = sklearn.datasets.load_mlcomp(\"20news-18828\", \"train\", mlcomp_root=MLCOMP_DIR)\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648\n"
     ]
    }
   ],
   "source": [
    "#Test Set \n",
    "test_data = sklearn.datasets.load_mlcomp(\"20news-18828\", \"test\", mlcomp_root=MLCOMP_DIR)\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3414\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For simplicity's sake, we will restrict ourselves to only some newsgroups so that the \n",
    "overall experimentation cycle is shorter. We can achieve this with the categories \n",
    "parameter as follows:\"\"\"\n",
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.ma c.hardware', 'comp.windows.x',\n",
    "          'sci.space']\n",
    "train_data = sklearn.datasets.load_mlcomp(\"20news-18828\", \"train\", mlcomp_root=MLCOMP_DIR, categories=groups)\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 18828, #features: 15924\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Real data is noisy. The newsgroup \n",
    "dataset is no exception. It even contains invalid characters that will result in \n",
    "UnicodeDecodeError.\n",
    "We have to tell the vectorizer to ignore them:\"\"\"\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "#vectorizer = StemmedTfidfVectorizer(min_df = 1, stop_words = 'english')\n",
    " \n",
    "#export LC_CTYPE=\"en_US.UTF-8\"\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, stop_words='english',decode_error = 'ignore')\n",
    "vectorized = vectorizer.fit_transform(data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We now have a pool of 18,828 posts and extracted for each of them a feature vector of \\n15,924 dimensions. That is what KMeans takes as input. We will fix the cluster size to \\n50 as shown in the following code:'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We now have a pool of 18,828 posts and extracted for each of them a feature vector of \n",
    "15,924 dimensions. That is what KMeans takes as input. We will fix the cluster size to \n",
    "50 as shown in the following code:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 33837.414\n",
      "Iteration  1, inertia 17962.825\n",
      "Iteration  2, inertia 17786.917\n",
      "Iteration  3, inertia 17690.956\n",
      "Iteration  4, inertia 17643.360\n",
      "Iteration  5, inertia 17616.664\n",
      "Iteration  6, inertia 17600.725\n",
      "Iteration  7, inertia 17576.889\n",
      "Iteration  8, inertia 17561.024\n",
      "Iteration  9, inertia 17554.252\n",
      "Iteration 10, inertia 17551.381\n",
      "Iteration 11, inertia 17548.903\n",
      "Iteration 12, inertia 17546.374\n",
      "Iteration 13, inertia 17544.371\n",
      "Iteration 14, inertia 17542.687\n",
      "Iteration 15, inertia 17541.061\n",
      "Iteration 16, inertia 17539.893\n",
      "Iteration 17, inertia 17539.054\n",
      "Iteration 18, inertia 17538.136\n",
      "Iteration 19, inertia 17537.640\n",
      "Iteration 20, inertia 17537.415\n",
      "Iteration 21, inertia 17537.264\n",
      "Iteration 22, inertia 17537.216\n",
      "Iteration 23, inertia 17537.171\n",
      "Iteration 24, inertia 17537.148\n",
      "Iteration 25, inertia 17537.139\n",
      "Iteration 26, inertia 17537.134\n",
      "Converged at iteration 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='random', max_iter=300, n_clusters=50, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1,verbose=1) \n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18828L,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"That's it. After fitting, we can get the clustering information out of the members of \n",
    "km. For every vectorized post that has been fit, there is a corresponding integer label \n",
    "in km.labels_:\"\"\"\n",
    "km.labels_\n",
    "km.labels_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"We now put everything together and demonstrate our system for the following new \n",
    "post that we assign to the variable 'new_post': \"\"\"\n",
    "\n",
    "new_post = \"Torus is one the best multipurpose templates on the market. It is modern in every way. One the best features is the page builder that comes with it, allowing you to easily create a page without much effort. This feature is also awesome for non-developers. The modular design technique used in Torus, promotes customizability and easy branding. Choose Torus if you want to get your site up and running in as little time as possible.\"\n",
    "\n",
    "\"\"\"We will first have to vectorize this post before we \n",
    "predict its label as follows:\"\"\"\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Now that we have the clustering, we do not need to compare new_post_vec to all \n",
    "post vectors. Instead, we can focus only on the posts of the same cluster. Let us fetch \n",
    "their indices in the original dataset:\"\"\"\n",
    "\n",
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3349\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Using similar_indices, we then simply have to build a list of posts together with \n",
    "their similarity scores as follows:\"\"\"\n",
    "\n",
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, data.data[i]))\n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[len(similar)/2]\n",
    "show_at_3 = similar[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.3261219298701756, \"From: arie@eecs.umich.edu (Arie Covrigaru)\\nSubject: Re: HP DeskWriter 550C...Opinions? Feedback!\\n\\nI like it a lot. It is worth the additional expense. The only problem I\\nfound is with MS Word (5.1a). If you have a table the is longer than\\na page and the cells have a visible border, the last (bottom of page)\\nline on the first page will be missing. It makes no difference how\\nthe table is formatted. The worst is that it doesn't show up in word's\\npage view or page preview. \\n--\\n\\n\\nArie.\\n\\n=========================================================================\\n| Arie Covrigaru                 |  University of Michigan AI Lab       |\\n| Phone: (313)994-8887           |  Room 149, Advanced Technology Bldg. |\\n| Internet: arie@eecs.umich.edu  |  1101 Beal Ave., Ann Arbor, MI 48109 |\\n=========================================================================\\n\")\n"
     ]
    }
   ],
   "source": [
    "print show_at_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.408517517170303, 'From: jenk@microsoft.com (Jen Kilmer)\\nSubject: Re: sex education\\n\\nIn article <Apr.7.23.20.08.1993.14209@athos.rutgers.edu> mprc@troi.cc.rochester.edu (M. Price) writes:\\n>In <Apr.5.23.31.32.1993.23904@athos.rutgers.edu> jenk@microsoft.com (Jen Kilmer) writes:\\n>\\n>> Method                  Expected         Actual \\n>> ------                 Failure Rate    Failure Rate\\n>> Abstinence                 0%              0% \\n>\\n>\\n>    These figures don\\'t seem to take account of rape. Or is a woman who\\n>is raped considered not to have been abstaining?\\n\\nI no longer have the textbook, but abstinence was defined as something\\nlike \"no contact between the penis and the vagina, vulva, or area \\nimmediately surrounding the vulva, and no transfer of semen to the\\nvagina, vulva, or area surrounding the vulva\".  \\n\\nThat is, abstinence wasn\\'t discussed as \"sex outside of marriage is\\nmorally wrong\" but as keep  the sperm away from the ovum and conception \\nis impossible. The moral question I recall the teacher asking was,\\n\"is it okay to create a child if you aren\\'t able to be a good parent\\nyet?\"\\n\\n-jen\\n\\n-- \\n\\n#include <stdisclaimer>  //  jenk@microsoft.com  // msdos testing\\n')\n"
     ]
    }
   ],
   "source": [
    "print show_at_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.4142135623730954, 'Subject: Quotation? Lowest bidder...\\nFrom: bioccnt@otago.ac.nz\\n\\n\\nCan someone please remind me who said a well known quotation? \\n\\nHe was sitting atop a rocket awaiting liftoff and afterwards, in answer to\\nthe question what he had been thinking about, said (approximately) \"half a\\nmillion components, each has to work perfectly, each supplied by the lowest\\nbidder.....\" \\n\\nAttribution and correction of the quote would be much appreciated. \\n\\nClive Trotman\\n\\n')\n"
     ]
    }
   ],
   "source": [
    "print show_at_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
